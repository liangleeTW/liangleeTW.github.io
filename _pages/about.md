---
permalink: /
title: "Academic Pages is a ready-to-fork GitHub Pages template for academic personal websites"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I’m Liang Lee, a PhD student working at the intersection of artificial intelligence and cognitive neuroscience. My research interests include large language models (LLMs), retrieval-augmented generation (RAG), explainable AI, and multisensory integration in motor adaptation. Currently, I’m exploring how insights from human cognition can guide the development of more transparent and human-aligned AI systems.


Academic Background
======
I’m pursuing a PhD in Neuroscience at National Yang Ming Chiao Tung University (NYCU), where I study brain connectivity and plasticity, particularly in relation to sensory recalibration and decision-making. I also hold a Master’s degree in Cognition and Science Learning, with training in big data analysis and deep learning, and a Bachelor’s in Electrophysics. This interdisciplinary path allows me to approach research questions with both computational precision and cognitive depth.

AI & Educational Technology
======
As an AI researcher in NYCU’s digital transformation project, I work on adapting large language models for educational applications—designing AI tutors, generating personalized feedback, and developing adaptive testing systems. In my previous role as a software engineer at [HERO](https://hero.nycu.edu.tw/), I built LLM-driven tools for MOOCs, analyzed large-scale learning behavior, and visualized educational data to support evidence-based instruction.

Cognitive Neuroscience Experience
======
During my master’s research at NYCU, I developed an explainable deep learning framework to identify where students focus their attention during online science assessments. By combining eye-tracking data with bidirectional LSTM (BiLSTM) models, I trained a system that not only predicts student performance with high accuracy (AUC: 0.88) but also highlights the specific textual and visual areas of interest (AOIs) that support correct answers. I applied integrated gradients to interpret the model’s predictions, allowing us to visualize which content features most contributed to successful learning. Experimental results showed that students who received AOI highlights performed significantly better and directed their attention more precisely, demonstrating the potential of deep learning–driven gaze-based feedback in real-time digital learning environments.

Teaching & Outreach
======
I’ve taught high school physics, run Python workshops, and assisted in university-level courses on morality and cognition. I also regularly give invited talks on topics such as AI ethics, generative AI in education, and the future of human-AI collaboration, reaching audiences from senior high school students to university researchers.

Skills at a Glance
======
I code primarily in Python and R, with experience in Arduino and MATLAB. My technical toolkit includes libraries such as PyTorch, LangChain, OpenCV, and scikit-learn. Some of my current projects involve edge device applications, including Raspberry Pi and Jetson Nano, and their integration with AI-driven sensing and control systems. I'm part of a team building a prism adaptation paradigm from scratch using Arduino and a custom mechanical setup, enabling precise control over timing and feedback in sensorimotor experiments. My core strength lies in bridging cognitive science insights with technical execution, aiming to create interpretable and human-centered AI systems.